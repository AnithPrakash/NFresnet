{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Optional, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code defines a custom 1d conv layer class and help to normalize the weights of the convolutional layers\n",
    "\n",
    "\n",
    "class WSConv1d(nn.Conv1d):\n",
    "    r\"\"\"applies a 1D convolutional over an input signal composed of several input\n",
    "    planes.\n",
    "    This module supports\n",
    "    * :attr:`stride` controls the stride for the cross-correlation, a single\n",
    "      number or a one-element tuple.\n",
    "    * :attr:`padding` controls the amount of implicit zero-paddings on both sides\n",
    "      for :attr:`padding` number of points.\n",
    "    * :attr:`dilation` controls the spacing between the kernel points; also\n",
    "      known as the Ã  trous algorithm. It is harder to describe, but this `link`_\n",
    "      has a nice visualization of what :attr:`dilation` does.\n",
    "    * :attr:`groups` controls the connections between inputs and outputs.\n",
    "      :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
    "      :attr:`groups`. For example,\n",
    "        * At groups=1, all inputs are convolved to all outputs.\n",
    "        * At groups=2, the operation becomes equivalent to having two conv\n",
    "          layers side by side, each seeing half the input channels,\n",
    "          and producing half the output channels, and both subsequently\n",
    "          concatenated.\n",
    "        * At groups= :attr:`in_channels`, each input channel is convolved with\n",
    "          its own set of filters,\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels, out_channels, kernal_size, stride=1, padding=0, dilation=1, groups=1, bais=True, padding_mode='zeros'):\n",
    "        super().__init__(in_channels,out_channels, kernal_size,stride=stride, padding=padding,\n",
    "                         dilation=dilation, groups=groups, bais=bais, padding_mode=padding_mode)\n",
    "        nn.init.kaiming_normal_(self.weight)\n",
    "        self.gain = nn.Parameter(torch.ones(\n",
    "            self.weight.size()[0], requires_grad=True\n",
    "        ))\n",
    "\n",
    "    # the function help to calculate the mean and variance of the weight \n",
    "    def standardize_weights(self,eps):\n",
    "        mean=torch.mean(self.weight, dim=(1,2), keepdim=True)\n",
    "        var= torch.std(self.weight, dim=(1,2), keepdim=True, unbiased=False)**2\n",
    "        fan_in= torch.prod(torch.tensor(self.weight.shape))\n",
    " \n",
    "        #fan in : cal the product of the weight dimensions \n",
    "        scale= torch.rsqrt(torch.max(\n",
    "            var * fan_in,torch.tensor(eps).to(var.device))) * self.gain.view_as(var).to(var.device)\n",
    "        #scale and shift : compute the scaling factor and shift for weights standardization\n",
    "        shift = mean * scale \n",
    "        return self.weight * scale - shift \n",
    "    \n",
    "    def forward(self, input, eps=1e-4):\n",
    "        weight=self.standardize_weights(eps)\n",
    "        return F.conv1d(input,weight,self.bias,self.stride,self.padding,self.dilation,self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSCon2d(nn.Conv2d):\n",
    "    \"\"\"Applies a 2D convo over an input signal composed \n",
    "    of several input planes after wieght normalization\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding,\n",
    "                         dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.weight)\n",
    "        self.gain = nn.Parameter(torch.ones(\n",
    "            self.weight.size(0), requires_grad=True))\n",
    "        \n",
    "    def standardize_weights(self, eps):\n",
    "        mean= torch.mean(self.weight, dim=(1, 2, 3), keepdim=True)\n",
    "        var= torch.std(self.weight, dim=(1,2), keepdim=True, unbiased=False) ** 2\n",
    "        fan_in=torch.prod(torch.tensor(self.weight.shape))\n",
    "\n",
    "        scale=torch.rsqrt(torch.max(\n",
    "            var * fan_in, torch.tensor(eps).to(var.device))) * self.gain.view_as(var).to(var.device)\n",
    "        shift=mean * scale \n",
    "        return self.weight * scale - shift\n",
    "    \n",
    "    def forward(self, input, eps=1e-4):\n",
    "        weight= self.standardize_weights(eps)\n",
    "        return F.conv1d(input, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConvTranspose2d(nn.ConvTranspose2d):\n",
    "    \"\"\"Applies a 2D transposed convolution operator over an input image\n",
    "    composed of several input planes after weight normalization/standardization.\"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 output_padding=0,\n",
    "                 groups: int = 1,\n",
    "                 bias: bool = True,\n",
    "                 dilation: int = 1,\n",
    "                 padding_mode: str = 'zeros'):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding,\n",
    "                         output_padding=output_padding, groups=groups, bias=bias, dilation=dilation, padding_mode=padding_mode)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.weight)\n",
    "        self.gain = nn.Parameter(torch.ones(\n",
    "            self.weight.size(0), requires_grad=True))\n",
    "        \n",
    "    def standardize_weights(self, eps):\n",
    "        mean = torch.mean(self.weight, dims=(1,2,3), keepdim=True)\n",
    "        var= torch.std(self.weight, dim=(1,2,3), keepdim=True) ** 2\n",
    "        fan_in= torch.prod(torch.tensor(self.weight.shape[1:]))\n",
    "\n",
    "        scale = torch.rsqrt(torch.max(\n",
    "            var * fan_in, torch.tensor(eps).to(var.device))) * self.gain.view_as(var).to(var.device)\n",
    "        shift = mean * scale\n",
    "        return self.weight * scale - shift\n",
    "\n",
    "    def forward(self, input: Tensor, output_size: Optional[List[int]] = None, eps: float = 1e-4) -> Tensor:\n",
    "        weight = self.standardize_weight(eps)\n",
    "        return F.conv_transpose2d(input, weight, self.bias, self.stride, self.padding, self.output_padding, self.groups, self.dilation)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledStdConv2d(nn.Conv2d):\n",
    "    \"\"\"Conv2d layer with scaled weight standardization\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride= 1, padding = 0, dilation = 1, groups = 1, bias= True, gain=True,gamma=1.0,eps=1e-5,use_layernorm=False) -> None:\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.gain = nn.Parameter(torch.ones(\n",
    "            self.out_channels,1,1,1\n",
    "        )) if gain else None\n",
    "        self.scale = gamma * self.weight[0].numel() ** -0.5\n",
    "        self.eps= eps ** 2 if use_layernorm else eps\n",
    "        self.use_layernorm = use_layernorm\n",
    "\n",
    "    def get_weight(self):\n",
    "        if self.use_layernorm:\n",
    "            weight = self.scale * \\\n",
    "                F.layer_norm(self.weight, self.weight.shape[1:], eps=self.eps)\n",
    "        else:\n",
    "            mean = torch.mean(\n",
    "                self.weight, dim=[1, 2, 3], keepdim=True)\n",
    "            std = torch.std(\n",
    "                self.weight, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
    "            weight = self.scale * (self.weight - mean) / (std + self.eps)\n",
    "        if self.gain is not None:\n",
    "            weight = weight * self.gain\n",
    "        return weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.get_weight(), self.bias, self.stride, self.padding, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from function.base import WSCon2d\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help to replace Conv2d layer with a custom layers\n",
    "def replace_conv(module : nn.Module, conv_class:WSCon2d):\n",
    "    \"\"\"Recursively replaces every convolution with WSConv2d\"\"\"\n",
    "    warnings.warn(\"Make sure to use it with non-residual model only\")\n",
    "    for name, mod in module.named_children():\n",
    "        target_mod= getattr(module, name)\n",
    "        if type(mod) == torch.nn.Conv2d:\n",
    "            setattr(module, name, conv_class(target_mod.in_channels, target_mod.out_channels, target_mod.kernel_size,\n",
    "                                           target_mod.stride, target_mod.padding, target_mod.dilation, target_mod.groups, target_mod.bias is not None))\n",
    "            \n",
    "        if type(mod) == torch.nn.BatchNorm2d:\n",
    "            setattr(module, name, torch.nn.Identity())\n",
    "\n",
    "    for name, mod in module.named_children():\n",
    "        replace_conv(mod, conv_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unitwise_norm(x:torch.Tensor):\n",
    "    if x.dim <=1:\n",
    "        dim=0\n",
    "        keepdim=False\n",
    "    elif x.ndim in [2, 3]:\n",
    "        dim = 0\n",
    "        keepdim = True\n",
    "    elif x.ndim == 4:\n",
    "        dim = [1, 2, 3]\n",
    "        keepdim = True\n",
    "    else:\n",
    "        raise ValueError('Wrong input dimensions')\n",
    "\n",
    "    return torch.sum(x**2, dim=dim, keepdim=keepdim) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch import optim, nn\n",
    "from function.utils import unitwise_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating a Stochasitic Gradient Descent with momentum, weight decay, Nesterov momentum and adaptive gradient clipping\n",
    "class SGD_AGC(Optimizer):\n",
    "    \"\"\"Implements stochastic gradient  descent \"\"\"\n",
    "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False, clipping=1e-2, eps=1e-3):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\n",
    "                \"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if clipping < 0.0:\n",
    "            raise ValueError(\"Invalid clipping value: {}\".format(clipping))\n",
    "        if eps < 0.0:\n",
    "            raise ValueError(\"Invalid eps value: {}\".format(eps))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov, clipping=clipping, eps=eps)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\n",
    "                \"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(SGD_AGC, self).__init__(params, defaults)\n",
    "\n",
    "#the optimizer maintains compatibility with older saved states\n",
    "    def __setstate__(self, state):\n",
    "        super(SGD_AGC, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "# code for adaptive gradient clipping\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                param_norm = torch.max(unitwise_norm(\n",
    "                    p.detach()), torch.tensor(group['eps']).to(p.device))\n",
    "                grad_norm = unitwise_norm(p.grad.detach())\n",
    "                max_norm = param_norm * group['clipping']\n",
    "\n",
    "                trigger = grad_norm > max_norm\n",
    "\n",
    "                clipped_grad = p.grad * \\\n",
    "                    (max_norm / torch.max(grad_norm,\n",
    "                                          torch.tensor(1e-6).to(grad_norm.device)))\n",
    "                p.grad.detach().copy_(torch.where(trigger, clipped_grad, p.grad))\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay)\n",
    "                if momentum != 0:\n",
    "                    param_state = self.state[p]\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(\n",
    "                            d_p).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(buf, alpha=momentum)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                p.add_(d_p, alpha=-group['lr'])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn, optim \n",
    "\n",
    "from function.utils import unitwise_norm\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Adaptive gradient clipping \n",
    "\n",
    "class AGC(optim.Optimizer):\n",
    "    \"\"\"Generic implmentation of the Adaptive Gradient Clipping\"\"\"\n",
    "\n",
    "    def __init__(self, params, optim: optim.Optimizer, clipping: float = 1e-2, eps: float = 1e-3, model=None, ignore_agc=[\"fc\"]):\n",
    "        if clipping < 0.0:\n",
    "            raise ValueError(\"Invalid clipping value: {}\".format(clipping))\n",
    "        if eps < 0.0:\n",
    "            raise ValueError(\"Invalid eps value: {}\".format(eps))\n",
    "        \n",
    "        self.optim=optim\n",
    "\n",
    "        defaults= dict(clipping=clipping, eps=eps)\n",
    "        defaults ={**defaults, **optim.defaults}\n",
    "\n",
    "        if not isinstance(ignore_agc, Iterable):\n",
    "            ignore_agc = [ignore_agc]\n",
    "\n",
    "        if model is not None:\n",
    "            assert ignore_agc not in [\n",
    "                None, []], \"You must specify ignore_agc for AGC to ignore fc-like(or other) layers\"\n",
    "            names = [name for name, module in model.named_modules()]\n",
    "\n",
    "            for module_name in ignore_agc:\n",
    "                if module_name not in names:\n",
    "                    raise ModuleNotFoundError(\n",
    "                        \"Module name {} not found in the model\".format(module_name))\n",
    "            params = [{\"params\": list(module.parameters())} for name,\n",
    "                          module in model.named_modules() if name not in ignore_agc]\n",
    "        \n",
    "        else:\n",
    "            params = [{\"params\": params}]\n",
    "\n",
    "        self.agc_params = params\n",
    "        self.eps = eps\n",
    "        self.clipping = clipping\n",
    "        \n",
    "        self.param_groups = optim.param_groups\n",
    "        self.state = optim.state\n",
    "\n",
    "        #super(AGC, self).__init__([], defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.agc_params:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param_norm = torch.max(unitwise_norm(\n",
    "                    p.detach()), torch.tensor(self.eps).to(p.device))\n",
    "                grad_norm = unitwise_norm(p.grad.detach())\n",
    "                max_norm = param_norm * self.clipping\n",
    "\n",
    "                trigger = grad_norm > max_norm\n",
    "\n",
    "                clipped_grad = p.grad * \\\n",
    "                    (max_norm / torch.max(grad_norm,\n",
    "                                          torch.tensor(1e-6).to(grad_norm.device)))\n",
    "                p.grad.detach().data.copy_(torch.where(trigger, clipped_grad, p.grad))\n",
    "\n",
    "        return self.optim.step(closure)\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = False):\n",
    "        r\"\"\"Sets the gradients of all optimized :class:`torch.Tensor` s to zero..\n",
    "        \"\"\"\n",
    "        for group in self.agc_params:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    if set_to_none:\n",
    "                        p.grad = None\n",
    "                    else:\n",
    "                        if p.grad.grad_fn is not None:\n",
    "                            p.grad.detach_()\n",
    "                        else:\n",
    "                            p.grad.requires_grad_(False)\n",
    "                        p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from function.base import WSCon2d, ScaledStdConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcite(nn.Module):\n",
    "  \n",
    "  def __init__(self, in_channels, out_channels, se_ratio=0.5, hidden_channels=None, activation='relu'):\n",
    "    assert (se_ratio != None) or ((se_ratio is None) and (hidden_channels is not None))\n",
    "    \n",
    "    if se_ratio is None:\n",
    "      hidden_channels = hidden_channels\n",
    "    else:\n",
    "      hidden_channels = max(1, se_ratio * in_channels)\n",
    "      \n",
    "    self.fc0 = nn.Linear(in_channels, hidden_channels)\n",
    "    self.fc1 = nn.Linear(hidden_channels, out_channels)\n",
    "    \n",
    "    self.activation  = activation_fn[activation]\n",
    "    super(SqueezeExcite, self).__init__()\n",
    "    \n",
    "  def forward(self, x):\n",
    "    h = torch.mean(x, [2,3])\n",
    "    h = self.fc0(h)\n",
    "    h = self.fc1(self.activation(h))\n",
    "    \n",
    "    return h.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFBlock(nn.Module):\n",
    "  \n",
    "  def __init__(self, in_channels, out_channels, expansion=0.5, se_ratio=0.5, kernel_shape=3, group_size=128, stride=1, beta=1.0, alpha=0.2, conv=ScaledStdConv2d, activation='gelu'):\n",
    "    \n",
    "    width = int(self.out_channels * expansion)\n",
    "    self.groups = width // group_size\n",
    "    self.width = group_size * self.groups\n",
    "    \n",
    "    self.conv0 = conv(in_channels, self.width, 1)\n",
    "    \n",
    "    self.conv1 = conv(self.width, self.width, 3, groups=self.groups)\n",
    "    \n",
    "    self.alpha = alpha\n",
    "    self.beta = beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "from function.base import WSCon2d, ScaledStdConv2d\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__=[\"nf_resnet18\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nonlin_gamma = dict(\n",
    "    identity=1.0,\n",
    "    celu=1.270926833152771,\n",
    "    elu=1.2716004848480225,\n",
    "    gelu=1.7015043497085571,\n",
    "    leaky_relu=1.70590341091156,\n",
    "    log_sigmoid=1.9193484783172607,\n",
    "    log_softmax=1.0002083778381348,\n",
    "    relu=1.7139588594436646,\n",
    "    relu6=1.7131484746932983,\n",
    "    selu=1.0008515119552612,\n",
    "    sigmoid=4.803835391998291,\n",
    "    silu=1.7881293296813965,\n",
    "    softsign=2.338853120803833,\n",
    "    softplus=1.9203323125839233,\n",
    "    tanh=1.5939117670059204,\n",
    ")\n",
    "\n",
    "ignore_inplace = ['gelu', 'silu', 'softplus', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fn = {\n",
    "    'identity': lambda x, *args, **kwargs: nn.Identity(*args, **kwargs)(x) * _nonlin_gamma['identity'],\n",
    "    'celu': lambda x, *args, **kwargs: nn.CELU(*args, **kwargs)(x) * _nonlin_gamma['celu'],\n",
    "    'elu': lambda x, *args, **kwargs: nn.ELU(*args, **kwargs)(x) * _nonlin_gamma['elu'],\n",
    "    'gelu': lambda x, *args, **kwargs: nn.GELU(*args, **kwargs)(x) * _nonlin_gamma['gelu'],\n",
    "    'leaky_relu': lambda x, *args, **kwargs: nn.LeakyReLU(*args, **kwargs)(x) * _nonlin_gamma['leaky_relu'],\n",
    "    'log_sigmoid': lambda x, *args, **kwargs: nn.LogSigmoid(*args, **kwargs)(x) * _nonlin_gamma['log_sigmoid'],\n",
    "    'log_softmax': lambda x, *args, **kwargs: nn.LogSoftmax(*args, **kwargs)(x) * _nonlin_gamma['log_softmax'],\n",
    "    'relu': lambda x, *args, **kwargs: nn.ReLU(*args, **kwargs)(x) * _nonlin_gamma['relu'],\n",
    "    'relu6': lambda x, *args, **kwargs: nn.ReLU6(*args, **kwargs)(x) * _nonlin_gamma['relu6'],\n",
    "    'selu': lambda x, *args, **kwargs: nn.SELU(*args, **kwargs)(x) * _nonlin_gamma['selu'],\n",
    "    'sigmoid': lambda x, *args, **kwargs: nn.Sigmoid(*args, **kwargs)(x) * _nonlin_gamma['sigmoid'],\n",
    "    'silu': lambda x, *args, **kwargs: nn.SiLU(*args, **kwargs)(x) * _nonlin_gamma['silu'],\n",
    "    'softplus': lambda x, *args, **kwargs: nn.Softplus(*args, **kwargs)(x) * _nonlin_gamma['softplus'],\n",
    "    'tanh': lambda x, *args, **kwargs: nn.Tanh(*args, **kwargs)(x) * _nonlin_gamma['tanh'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1, base_conv: nn.Conv2d = ScaledStdConv2d) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return base_conv(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1, base_conv: nn.Conv2d = ScaledStdConv2d) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return base_conv(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        alpha: float = 0.2,\n",
    "        beta: float = 1.0,\n",
    "        activation: str = 'relu',\n",
    "        base_conv: nn.Conv2d = ScaledStdConv2d\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\n",
    "                'BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\n",
    "                \"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride, base_conv=base_conv)\n",
    "        self.activation = activation\n",
    "        \n",
    "        if activation not in ignore_inplace:\n",
    "            self.act = partial(activation_fn[activation], inplace=True)\n",
    "        else:\n",
    "            self.act = partial(activation_fn[activation])\n",
    "        self.conv2 = conv3x3(planes, planes, base_conv=base_conv)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        out = activation_fn[self.activation](x=x) * self.beta\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.act(x=out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out *= self.alpha\n",
    "        out += identity\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        alpha: float = 0.2,\n",
    "        beta: float = 1.0,\n",
    "        activation: str = 'relu',\n",
    "        base_conv: nn.Conv2d = ScaledStdConv2d,\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width, base_conv=base_conv)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups,\n",
    "                             dilation, base_conv=base_conv)\n",
    "        self.conv3 = conv1x1(\n",
    "            width, planes * self.expansion, base_conv=base_conv)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.activation = activation\n",
    "        if activation not in ignore_inplace:\n",
    "            self.act = partial(activation_fn[activation], inplace=True)\n",
    "        else:\n",
    "            self.act = partial(activation_fn[activation])\n",
    "        \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        out = activation_fn[self.activation](x) * self.beta\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.act(x=out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act(x=out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out *= self.alpha\n",
    "        out += identity\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFResNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        alpha: float = 0.2,\n",
    "        beta: float = 1.0,\n",
    "        activation: str = 'relu',\n",
    "        base_conv: nn.Conv2d = ScaledStdConv2d\n",
    "    ) -> None:\n",
    "        super(NFResNet, self).__init__()\n",
    "        \n",
    "        assert activation in activation_fn.keys()\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = base_conv(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(\n",
    "            block, 64, layers[0], alpha=alpha, beta=beta, activation=activation, base_conv=base_conv)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0], alpha=alpha, beta=beta, activation=activation, base_conv=base_conv)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1], alpha=alpha, beta=beta, activation=activation, base_conv=base_conv)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2], alpha=alpha, beta=beta, activation=activation, base_conv=base_conv)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='linear')\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    # type: ignore[arg-type]\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    # type: ignore[arg-type]\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False, alpha: float = 0.2, beta: float = 1.0, activation: str = 'relu', base_conv: nn.Conv2d = ScaledStdConv2d) -> nn.Sequential:\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion,\n",
    "                        stride, base_conv=base_conv),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, alpha=alpha, beta=beta, activation=activation, base_conv=base_conv))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                alpha=alpha, beta=beta, activation=activation,\n",
    "                                base_conv=base_conv))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nf_resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    alpha: float,\n",
    "    beta: float,\n",
    "    activation: str,\n",
    "    base_conv: nn.Conv2d,\n",
    "    **kwargs: Any\n",
    ") -> NFResNet:\n",
    "    model = NFResNet(block, layers, alpha=alpha, beta=beta, activation=activation, base_conv=base_conv, **kwargs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_resnet18(alpha: float = 0.2, beta: float = 1.0, activation: str = 'relu', base_conv: nn.Conv2d = ScaledStdConv2d, **kwargs: Any) -> NFResNet:\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    and `\"High-Performance Large-Scale Image Recognition Without Normalization\" <https://arxiv.org/pdf/2102.06171v1>`.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    return _nf_resnet('resnet18', BasicBlock, [2, 2, 2, 2], alpha=alpha, beta=beta, activation=activation, base_conv=base_conv,\n",
    "                      **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
